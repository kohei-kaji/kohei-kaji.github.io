---
title: 'n-gram è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’å‹‰å¼·ã™ã‚‹'
date: '2025-06-30'
author_profile: false
permalink: /posts/2025/ngram/
---

\\( n \\)-gram è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ã¡ã‚‡ã£ã¨ç†è§£ã—ãŸã„ã€ã¨ã„ã†æ°—æŒã¡ã®è¨˜äº‹ã§ã™ã€‚


å˜èªåˆ— \\( w_1,\cdots,w_n \\) ã®ç¢ºç‡ \\( P(w_1,\cdots,w_n) \\) ã¯ã€\\( \prod^n_i P(w_i\mid w_1,\cdots,w_{i-1}) \\) ã§æ±‚ã‚ã‚‰ã‚Œã¾ã™ã€‚
\\( n \\)-gram è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã¯ã€ã“ã®æ¡ä»¶ä»˜ãç¢ºç‡ \\( P(w\mid \mathbf{c}) \\) ã®æ¡ä»¶éƒ¨åˆ†ï¼ˆï¼æ–‡è„ˆ \\( \mathbf{c} \\) ï¼‰ã®é•·ã•ã‚’ \\( n-1 \\) ã«å›ºå®šã—ãŸã‚‚ã®ã§ã™ã€‚

æ¡ä»¶ä»˜ãç¢ºç‡ \\( P(w\mid \mathbf{c}) \\) ã¯ã€ã‚ã‚‹å˜èªåˆ—ã®ã‚³ãƒ¼ãƒ‘ã‚¹ä¸Šã®å‡ºç¾é »åº¦ã‚’ \\( C(\cdot) \\) ã¨ã—ã¦ã€
\\[
  P(w\mid \mathbf{c}) = \frac{C(\mathbf{c}w)}{C(\mathbf{c}\star)}  
\\]
ã§æ±‚ã‚ã‚‰ã‚Œã¾ã™ã€‚\\( \star \\) ã¯ä»»æ„ã®å˜èªã§ã™ã€‚

ãŸã ã—ã€å˜ç´”ã«å‡ºç¾é »åº¦ã‚’ã‚‚ã¨ã«æœ€å°¤æ¨å®šã§ç¢ºç‡ã‚’æ±‚ã‚ã‚‹ã€ã¨ã„ã†ã®ã§ã¯ã€å‡ºç¾é »åº¦ã‚’æ•°ãˆã‚‹ã®ã«ç”¨ã„ãŸã‚³ãƒ¼ãƒ‘ã‚¹ã«ä¾å­˜ã—ã€ç¢ºç‡ã‚’éå¤§ãƒ»éå°è©•ä¾¡ã—ã¦ã—ã¾ã†ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚
ç‰¹ã«ã€\\( n \\) ãŒå¤§ãã‘ã‚Œã°å¤§ãã„ã»ã©ã€\\( C(\mathbf{c}w) = 0 \\) ã¨ãªã‚‹å¯èƒ½æ€§ã¯é«˜ããªã‚Šã€\\( P(w\mid \mathbf{c}) = 0 \\) ã¨ãªã£ã¦ã—ã¾ã†ã€ãªã©ã€‚

ãã®ãŸã‚ã«ã€(i) \\( n \\)-gramã®å‡ºç¾é »åº¦ã‚’ä¸€å®šå€¤ $D$ ã ã‘å¼•ã (discount)ã€(ii) \\( n-1 \\) ä»¥ä¸‹ã®


## Kneser-Ney smoothing
- Kneser & Ney (1995) ã«ã‚ˆã‚‹ **çµ¶å¯¾å€¤ãƒ‡ã‚£ã‚¹ã‚«ã‚¦ãƒ³ãƒˆ & ãƒãƒƒã‚¯ã‚ªãƒ•æ³•**

* **ã‚³ã‚¢ãƒ»ã‚¢ã‚¤ãƒ‡ã‚¢**:

  1. **é«˜æ¬¡æ•° *n*-gram ã®ç”Ÿèµ·å›æ•°ã‚’ä¸€å®šå€¤ $D$ ã ã‘å¼•ãï¼ˆdiscountï¼‰**
  2. æ®‹ã‚Šã®ç¢ºç‡è³ªé‡ã‚’ **ã€Œæ–‡è„ˆã‚’è½ã¨ã—ãŸä½æ¬¡æ•°ãƒ¢ãƒ‡ãƒ«ã€ã¸åˆ†é…ï¼ˆback-off / interpolationï¼‰**
  3. ä½æ¬¡æ•°å´ã§ã¯ã€Œèª *w* ãŒ\*\*ä½•é€šã‚Šã®æ–‡è„ˆã§ç¶™ç¶šï¼ˆcontinueï¼‰\*\*ã—ã¦ç¾ã‚ã‚ŒãŸã‹ã€ã‚’ä½¿ã† ***continuation probability*** ã‚’æ¨å®šã™ã‚‹

å¼ã§æ›¸ãã¨ï¼ˆ2-gram ä»¥ä¸Šã®å ´åˆï¼‰

$$
P_{\text{KN}}(w_i\mid h)=
\frac{\max\{c(h\,w_i)-D,0\}}{c(h)}
+\lambda(h)\;P_{\text{cont}}(w_i)
$$

$$
P_{\text{cont}}(w)=
\frac{\#\{\text{æ–‡è„ˆ }h':c(h' w)>0\}}
     {\sum_{w'}\#\{\text{æ–‡è„ˆ }h':c(h' w')>0\}}
$$

ã“ã“ã§ $c(\cdot)$ ã¯ç”Ÿèµ·å›æ•°ï¼Œ$\lambda(h)=\frac{D\,N_1^+(h\*)}{c(h)}$ï¼ˆ$N_1^+$ ã¯èªå½™ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ç”¨ã®ç•°ã‚¿ã‚¤ãƒ—æ•°ï¼‰ã§ã™ã€‚([scribd.com][1])

---

## 2 ï¸âƒ£ è¨­è¨ˆæ€æƒ³ã¨ç‹™ã„

### ãªãœã€Œç¶™ç¶šå›æ•°ã€ãªã®ã‹

é »åº¦ã®é«˜ã„æ©Ÿèƒ½èª (â€œofâ€, â€œtheâ€) ã¯ bigram ã§ã¯é«˜é »åº¦ã§ã™ãŒã€Œå¾Œç¶šèªã¨ã—ã¦ã©ã“ã«ã§ã‚‚ç¾ã‚Œã‚‹ã€ã¨ã¯é™ã‚Šã¾ã›ã‚“ã€‚

* **Good-Turing ã‚„ Katz smoothing** ã§ã¯ **ä½æ¬¡æ•°ãƒ¢ãƒ‡ãƒ«ãŒâ€œé »åº¦â€ã«å¼•ããšã‚‰ã‚Œã‚‹** â†’ èªã®å›ºæœ‰ãƒã‚¤ã‚¢ã‚¹ãŒæ®‹ã‚‹
* **KN** ã¯ **ã€Œã©ã‚Œã ã‘å¤šæ§˜ãªæ–‡è„ˆã§ç¶šãã‹ã€ã‚’ä½æ¬¡æ•°å´ã®ç¢ºç‡åŸºç›¤**ã«ã™ã‚‹ã“ã¨ã§ã€ã“ã®ãƒã‚¤ã‚¢ã‚¹ã‚’æŠ‘ãˆã€é«˜é »åº¦èªã§ã‚‚ç¢ºç‡ã‚’éâ¼¤è©•ä¾¡ã—ã«ãã„





### 1. 2-gram Kneser-Ney ã®å¼ã‚’åˆ†è§£ã—ã¦ã¿ã‚‹

2-gramï¼ˆbigramï¼‰ã®å ´åˆï¼Œæ­´å² $h=w_{i-1}$ ã®ã‚ã¨ã«èª $w_i$ ãŒç¶šãç¢ºç‡ã¯

$$
P_{\text{KN}}(w_i\mid h)=
\underbrace{\frac{\max\{c(hw_i)-D,0\}}{c(h)}}_{\text{â‘  å‰²å¼•å¾Œã®ç›¸å¯¾é »åº¦}}+
\underbrace{\lambda(h)}_{\text{â‘¡ ä½™å‰°ç¢ºç‡è³ªé‡}\;}
\underbrace{P_{\text{cont}}(w_i)}_{\text{â‘¢ ç¶™ç¶šç¢ºç‡}}
$$

* **â‘  å‰²å¼•å¾Œã®ç›¸å¯¾é »åº¦**

  * $c(hw_i)$ ã¯ **bigram $h w_i$** ã®è¨“ç·´ã‚³ãƒ¼ãƒ‘ã‚¹å‡ºç¾å›æ•°ã€‚
  * $D$ ã¯ **å®šæ•°ãƒ‡ã‚£ã‚¹ã‚«ã‚¦ãƒ³ãƒˆ**ï¼ˆé€šå¸¸ $0.5\!-\!1.0$ ç¨‹åº¦ï¼Œãƒ‡ãƒ¼ã‚¿ä¾å­˜ã§æœ€é©åŒ–ï¼‰ã€‚
  * ç›®çš„ã¯ **ã€Œè¦‹ãˆã¦ã„ã‚‹ n-gram ã‚’å°‘ã—ã ã‘æ¸›ã‚‰ã—ï¼Œæœªè¦³æ¸¬ n-gram ç”¨ã«ç¢ºç‡ã‚’ç©ºã‘ã‚‹ã€** ã“ã¨ã€‚
  * $\max\{\,\cdot,0\}$ ã§ **å‡ºç¾ 1 å›ã ã‘ã® n-gram ãŒãƒã‚¤ãƒŠã‚¹ã«ãªã‚‰ãªã„** ã‚ˆã†ã«ã™ã‚‹ã€‚
  * åˆ†æ¯ $c(h)$ ã§æ­£è¦åŒ– â†’ ã€Œå‰²å¼•å¾Œã§ã‚‚æ–‡è„ˆ $h$ ã«ãŠã‘ã‚‹ç¢ºç‡ãŒç·å’Œ 1ã€ã€‚
    ([foldl.me][1])

* **â‘¡ ä½™å‰°ç¢ºç‡è³ªé‡ $\lambda(h)$**

  * â‘ ã§ã€Œå¼•ã„ãŸã¶ã‚“ã€ã®ç·é‡ã‚’ **ä½æ¬¡æ•°ãƒ¢ãƒ‡ãƒ«ã¸æ¸¡ã™ãŸã‚ã®é‡ã¿**ã€‚
  * å®Ÿè£…ã§ã¯

    $$
      \lambda(h)=
        \frac{D \; N_{1}^{+}(h\star)}{c(h)}
    $$

    ãŸã ã— $N_{1}^{+}(h\star)$ ã¯ **æ–‡è„ˆ $h$ ã¨å…±èµ·ã—ãŸç•°ã‚¿ã‚¤ãƒ—æ•°**ï¼ˆèªå½™ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°ï¼‰ã€‚
  * **æ–‡è„ˆãŒå¤šæ§˜ãªã»ã©ä½™å‰°ã‚‚å¤§ãããªã‚‹**ã®ã§ï¼ŒæœªçŸ¥èªã¸ã®é…åˆ†ãŒè‡ªç„¶ã«å¢—ãˆã‚‹ã€‚

* **â‘¢ ç¶™ç¶šç¢ºç‡ $P_{\text{cont}}(w_i)$**

  * å˜èª $w$ ãŒã€Œ**ä½•é€šã‚Šã®å‰æ¥æ–‡è„ˆã§â€œå¾Œç¶šèªâ€ã¨ã—ã¦ç¾ã‚ŒãŸã‹**ã€ã«æ¯”ä¾‹ï¼š

    $$
      P_{\text{cont}}(w)=
        \frac{\#\{h':c(h'w)>0\}}
             {\sum_{w'}\#\{h':c(h' w')>0\}}
    $$
  * ã“ã‚Œã¯ **â€œé »åº¦â€ã§ã¯ãªãâ€œæ–‡è„ˆã®å¤šæ§˜æ€§â€** ã‚’æ¸¬ã‚‹æŒ‡æ¨™ã€‚
  * ä¾‹ï¼š
    *â€œFranciscoâ€* ã¯å‡ºç¾ç·æ•°ã¯å¤šã„ãŒ **ã»ã¼å¸¸ã« â€œSanâ€ ã®å¾Œ**ã«ç¶šã â†’ ç¶™ç¶šç¢ºç‡ã¯ä½ã„ã€‚
    *â€œWednesdayâ€* ã¯æ•°ã¯å°‘ãªã„ãŒ **å¤šå½©ãªå‰ç½®è©ã®å¾Œ**ã«ç¶šã â†’ ç¶™ç¶šç¢ºç‡ã¯æ¯”è¼ƒçš„é«˜ã„ã€‚
  * ã“ã‚Œã«ã‚ˆã‚Šï¼Œé »åº¦ã®é«˜ã„æ©Ÿèƒ½èªãŒä½æ¬¡æ•°å´ã§éå¤§è©•ä¾¡ã•ã‚Œã‚‹å•é¡Œã‚’å›é¿ã™ã‚‹ã€‚
    ([medium.com][2])

---

#### ğŸ”¢ ã”ãå°ã•ãªæ•°å€¤ä¾‹

| bigram          | å›æ•° |
| --------------- | -- |
| â€œSan Franciscoâ€ | 50 |
| â€œFrancisco isâ€  | 1  |
| â€œon Wednesdayâ€  | 5  |
| â€œat Wednesdayâ€  | 3  |

* â€œFranciscoâ€ ã®ç¶™ç¶šæ–‡è„ˆã¯ {San, Francisco} ã® 2 ç¨®ã®ã¿ â†’ ç¶™ç¶šç¢ºç‡ã¯ä½ã€‚
* â€œWednesdayâ€ ã¯ {on, at} ã® 2 ç¨®â€¦ã¨å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§ã‚‚ã€Œå¤šæ§˜æ€§ã€ãŒå–ã‚Œã‚‹ã€‚
  â†’ æœªçŸ¥ã®æ–‡è„ˆã§ â€œFranciscoâ€ ãŒç¾ã‚Œã‚‹äºˆæ¸¬ç¢ºç‡ã¯ â€œWednesdayâ€ ã‚ˆã‚Šä½ããªã‚‹ã€‚

---

### 2. Back-off ã¨ Interpolation ã¯åŒã˜ï¼Ÿ

* **Back-off**

  * è¦³æ¸¬ã•ã‚Œã¦ã„ã‚Œã° **é«˜æ¬¡æ•° n-gram ã®ã¿** ã‚’ä½¿ã„ï¼Œ0 ä»¶ãªã‚‰ $n\!-\!1$ gram ã¸ *è½ã¨ã™*ã€‚
  * å…¸å‹ä¾‹ï¼š**Katz back-off**ï¼›æ¤œç´¢ã‚³ã‚¹ãƒˆãŒä½ã„ã€‚([web.stanford.edu][3])

* **Interpolation**

  * **å¸¸ã«è¤‡æ•°æ¬¡æ•°ã‚’ç·šå½¢çµåˆ**ï¼š

    $$
      P = \alpha_3 P(3\text{-gram}) + \alpha_2 P(2\text{-gram}) + \alpha_1 P(1\text{-gram})
    $$
  * æœªè¦³æ¸¬ã§ã‚‚ $\alpha$ ã«å¿œã˜ã¦ä¸‹ä½ãƒ¢ãƒ‡ãƒ«ãŒå¯„ä¸ã™ã‚‹ã®ã§ **ã‚ˆã‚Šæ»‘ã‚‰ã‹**ã€‚

> **é•ã„**
>
> * è¦³æ¸¬ n-gram ãŒå­˜åœ¨ã™ã‚‹å ´åˆ
>
>   * *Back-off*: ä¸Šä½ã ã‘ä½¿ã†
>   * *Interpolation*: ä¸Šä½ã‚’ä¸­å¿ƒã«ä¸‹ä½ã‚‚æ··ãœã‚‹
> * **Modified-Kneser-Ney** ã¯ *Interpolation* æ–¹å¼ã€ã‚ªãƒªã‚¸ãƒŠãƒ« KN ã¯æ®µéšçš„ Back-offã€‚([scaler.com][4])

---

### 3. Pitman-Yor éç¨‹ã¨ Chinese Restaurant Process (CRP)

| ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰                          | ã²ã¨ã“ã¨ã§                                                       | å½¹å‰²                                                                                  |
| ------------------------------ | ----------------------------------------------------------- | ----------------------------------------------------------------------------------- |
| **Pitman-Yor process**         | Dirichlet éç¨‹ã®æ‹¡å¼µã€‚**å‰²å¼• $d$** ãŒã‚ã‚Š\*\* Zipf çš„ (ã¹ãä¹—)\*\* é »åº¦åˆ†å¸ƒã‚’ç”Ÿæˆ | è‡ªç„¶è¨€èªã®èªé »åº¦ã‚’ã†ã¾ãå†ç¾ã™ã‚‹ ([en.wikipedia.org][5])                                            |
| **Chinese Restaurant Process** | â€œç„¡é™ãƒ†ãƒ¼ãƒ–ãƒ«â€ ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ã«å®¢ï¼ˆãƒ‡ãƒ¼ã‚¿ï¼‰ã‚’é †ã«åº§ã‚‰ã›ã‚‹ç¢ºç‡ãƒ¢ãƒ‡ãƒ«                           | Pitman-Yor ã®ç”Ÿæˆæ‰‹é †ã‚’**å¸­å–ã‚Šã‚¢ãƒŠãƒ­ã‚¸ãƒ¼**ã§è¦–è¦šåŒ–ã€‚æ–°èªã¯æ–°ãƒ†ãƒ¼ãƒ–ãƒ«ã«ï¼Œæ—¢çŸ¥èªã¯äººæ°—ãƒ†ãƒ¼ãƒ–ãƒ«ã«é›†ã¾ã‚‹ã€‚ ([en.wikipedia.org][6]) |

#### ğŸ“Œ KN ã¨ Pitman-Yor ã®é–¢ä¿‚

* **éšå±¤ Pitman-Yorï¼ˆHPYPï¼‰** ã‚’ n-gram ã®å„æ–‡è„ˆé•·ã«é‡ã­ã‚‹ã¨

  1. å„æ–‡è„ˆ = 1 åº—èˆ—ï¼ˆãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ï¼‰
  2. å‰²å¼• $d$ â‰ˆ $D/c(h)$
  3. **HPYP ã®äºˆæ¸¬åˆ†å¸ƒ = Interpolated KN**ï¼ˆMAP æ¨å®šï¼‰ã«ãªã‚‹
     â†’ KN ã¯ **â€œé‡ã¿ä»˜ã HPYP ã®æœ€é »â€** ã‚’é–‰å½¢å¼ã§æ›¸ã„ãŸã‚‚ã®ã¨ç†è§£ã§ãã‚‹ã€‚([stats.ox.ac.uk][7])

---

### 4. ã¾ã¨ã‚ãƒ»è¦ç‚¹å†æ²

1. **2-gram å¼**ã®ï¼“é …ï¼š
   *â‘  å‰²å¼•å¾Œç›¸å¯¾é »åº¦*ï¼ˆè¦‹ãˆã¦ã„ã‚‹ã‚‚ã®ã‚’æ¸›ã‚‰ã™ï¼‰
   *â‘¡ ä½™å‰°è³ªé‡*ï¼ˆä¸‹ä½ãƒ¢ãƒ‡ãƒ«ã¸ã®æ©‹æ¸¡ã—ï¼‰
   *â‘¢ ç¶™ç¶šç¢ºç‡*ï¼ˆæ–‡è„ˆå¤šæ§˜æ€§ã§æœªçŸ¥ã‚’æ¨å®šï¼‰
2. **Back-off â‰  Interpolation**ï¼šå¾Œè€…ã¯å¸¸ã«æ··ãœã‚‹ç‚¹ãŒé•ã„ï¼ŒModified-KN ã¯å¾Œè€…ã€‚
3. **Pitman-Yorï¼CRP** ã¯ **ç¢ºç‡çš„ç”Ÿæˆãƒ¢ãƒ‡ãƒ«**ã€‚éšå±¤åŒ–ã™ã‚‹ã¨ KN ãŒãã® MAP æ¨å®šå½¢ã«ãªã‚‹ â€• ã™ãªã‚ã¡ **KN ã¯â€œãƒ™ã‚¤ã‚ºçš„ HPYP ã®è¿‘ä¼¼â€**ã€‚

ã“ã‚Œã§ã€Œå¼ã®å„éƒ¨ãŒä½•ã‚’ã—ã¦ã„ã‚‹ã‹ã€ã€ŒBack-off ã¨ Interpolation ã®é•ã„ã€ã€Œãƒ™ã‚¤ã‚ºçš„èƒŒæ™¯ã€ãŒã¤ãªãŒã‚‹ã¯ãšã§ã™ã€‚å¿…è¦ãŒã‚ã‚Œã°æ•°å€¤ä¾‹ã‚„ã‚³ãƒ¼ãƒ‰ä¾‹ã‚‚è¿½è£œã„ãŸã—ã¾ã™ã®ã§ãŠçŸ¥ã‚‰ã›ãã ã•ã„ã€‚

[1]: https://www.foldl.me/2014/kneser-ney-smoothing/?utm_source=chatgpt.com "Kneser-Ney smoothing explained - foldl"
[2]: https://medium.com/%40dennyc/a-simple-numerical-example-for-kneser-ney-smoothing-nlp-4600addf38b8?utm_source=chatgpt.com "A simple numerical example for Kneser-Ney Smoothing [NLP]"
[3]: https://web.stanford.edu/~jurafsky/slp3/3.pdf?utm_source=chatgpt.com "[PDF] N-gram Language Models - Stanford University"
[4]: https://www.scaler.com/topics/nlp/backoff-in-nlp/?utm_source=chatgpt.com "Backoff in NLP - Scaler Topics"
[5]: https://en.wikipedia.org/wiki/Pitman%E2%80%93Yor_process?utm_source=chatgpt.com "Pitmanâ€“Yor process"
[6]: https://en.wikipedia.org/wiki/Chinese_restaurant_process?utm_source=chatgpt.com "Chinese restaurant process"
[7]: https://www.stats.ox.ac.uk/~teh/research/compling/hpylm.pdf?utm_source=chatgpt.com "[PDF] A Bayesian Interpretation of Interpolated Kneser-Ney - statistics"
